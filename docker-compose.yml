services:
  # ───────────────────── MinIO ─────────────────────
  minio-dev:
    container_name: minio-dev
    image: minio/minio:RELEASE.2022-11-08T05-27-07Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9001:9000"
      - "9002:9001"
    volumes:
      - minio_data:/data

  minio-init:
    image: minio/mc
    depends_on:
      - minio-dev
    entrypoint: >
      sh -c "
        sleep 10 &&
        mc config host add myminio http://minio-dev:9000 minioadmin minioadmin &&
        mc mb myminio/ufc || echo 'Bucket already exists'
      "

  # ───────────────────── PostgreSQL ─────────────────────
  postgres_db:
    container_name: postgres_db
    image: postgres:12
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ufc_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    depends_on:
      - minio-dev

  # ───────────────────── Airflow log-dirs init ─────────────────────
  airflow-logs-init:
    image: apache/airflow:2.7.0
    container_name: airflow_logs_init
    user: root
    depends_on:
      - postgres_db
    entrypoint: >
      bash -c "
        mkdir -p /opt/airflow/logs/{scheduler,webserver,worker} &&
        chown -R 50000:50000 /opt/airflow/logs
      "
    volumes:
      - ./airflow-data/logs:/opt/airflow/logs

  # ───────────────────── Airflow database & users init ─────────────────────
  airflow-init:
    image: apache/airflow:2.7.0
    container_name: airflow_init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres_db:5432/ufc_db
      AIRFLOW__CORE__FERNET_KEY: FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      _PIP_ADDITIONAL_REQUIREMENTS: "beautifulsoup4 requests pandas s3fs pyspark"
      PYTHONPATH: "/opt/airflow:/opt/airflow/project"
    volumes:
      - .:/opt/airflow/project
      - ./dags:/opt/airflow/dags
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
    depends_on:
      - postgres_db
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # 1. Upgrade the metadata DB to the latest migration.
        airflow db upgrade

        # 2. If no users exist, create the default Admin.
        if ! airflow users list --output table | grep -q 'username'; then
          airflow users create \
            --role Admin \
            --username airflow \
            --password airflow \
            --email airflow@airflow.com \
            --firstname airflow \
            --lastname airflow
        fi
    restart: on-failure

  # ───────────────────── Airflow webserver ─────────────────────
  airflow-webserver:
    image: apache/airflow:2.7.0
    container_name: airflow_webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres_db:5432/ufc_db
      AIRFLOW__CORE__FERNET_KEY: FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      _PIP_ADDITIONAL_REQUIREMENTS: "beautifulsoup4 requests pandas s3fs pyspark"
      PYTHONPATH: "/opt/airflow:/opt/airflow/project"
    volumes:
      - .:/opt/airflow/project
      - ./dags:/opt/airflow/dags
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
    depends_on:
      - postgres_db
      - airflow-logs-init
      - airflow-init
    command: airflow webserver
    ports:
      - "8082:8080"
    restart: always

  # ───────────────────── Airflow scheduler ─────────────────────
  airflow-scheduler:
    image: apache/airflow:2.7.0
    container_name: airflow_scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres_db:5432/ufc_db
      AIRFLOW__CORE__FERNET_KEY: FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      _PIP_ADDITIONAL_REQUIREMENTS: "beautifulsoup4 requests pandas s3fs pyspark"
      PYTHONPATH: "/opt/airflow:/opt/airflow/project"
    volumes:
      - .:/opt/airflow/project
      - ./dags:/opt/airflow/dags
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
    depends_on:
      - postgres_db
      - airflow-logs-init
      - airflow-init
    command: airflow scheduler
    restart: always

  # ───────────────────── app container ─────────────────────
  app:
    build: .
    container_name: ufc_app
    volumes:
      - .:/app
    working_dir: /app
    command: tail -f /dev/null
    stdin_open: true
    tty: true
    depends_on:
      - minio-dev
      - airflow-webserver

volumes:
  minio_data:
  postgres_data: